from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
import aiohttp
import asyncio
import logging
from datetime import datetime
import os

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

app = FastAPI(title="Data Extraction Service", version="1.0.0")

# æ•°æ®æ¨¡å‹
class SiteConfig(BaseModel):
    name: str
    client_id: str
    key: str
    propertyId: str
    categoryId: str

class ExtractRequest(BaseModel):
    hasura_url: str
    start: str
    end: str
    propertyvalue: str
    all_sites: List[SiteConfig]

class DataExtractor:
    def __init__(self):
        self.session = None
        self.token_url = "https://api.kolable.app/api/v1/auth/token"  # å›ºå®štokenç«¯ç‚¹
        self.query_url = "https://rhdb.kolable.com/v1/graphql"       # å›ºå®šæŸ¥è¯¢ç«¯ç‚¹
        self.max_retries = 10
    
    async def create_session(self):
        if not self.session:
            timeout = aiohttp.ClientTimeout(total=30)
            self.session = aiohttp.ClientSession(timeout=timeout)
        return self.session
    
    async def close_session(self):
        if self.session:
            await self.session.close()
            self.session = None
    
    async def get_site_token(self, site_config: SiteConfig) -> Optional[str]:
        """ç²å–ç«™é»token - ä½¿ç”¨å›ºå®šç«¯é»"""
        session = await self.create_session()
        
        # æŒ‰ç…§ä½ æŒ‡å®šçš„æ ¼å¼æ§‹å»ºè«‹æ±‚
        token_payload = {
            "clientId": site_config.client_id,
            "key": site_config.key,
            "permission": []
        }
        
        for attempt in range(1, self.max_retries + 1):
            try:
                logger.info(f"[{site_config.name}] ç²å–token - å˜—è©¦ {attempt}/{self.max_retries}")
                logger.info(f"[{site_config.name}] è«‹æ±‚æ•¸æ“š: {token_payload}")
                
                async with session.post(
                    self.token_url,
                    json=token_payload,
                    headers={"Content-Type": "application/json"}
                ) as response:
                    
                    if response.status == 200:
                        result = await response.json()
                        # ä¿®æ­£ï¼štokenåœ¨result.authTokenè£¡é¢
                        token = None
                        if result.get("result") and result["result"].get("authToken"):
                            token = result["result"]["authToken"]
                        elif result.get("token"):  # å‚™ç”¨æ–¹æ¡ˆï¼Œä»¥é˜²å…¶ä»–ç«™é»æ ¼å¼ä¸åŒ
                            token = result["token"]
                        
                        if token:
                            logger.info(f"[{site_config.name}] âœ… æˆåŠŸç²å–token")
                            return token
                        else:
                            logger.warning(f"[{site_config.name}] âš ï¸ éŸ¿æ‡‰ä¸­æ²’æœ‰æ‰¾åˆ°token: {result}")
                    else:
                        error_text = await response.text()
                        logger.warning(f"[{site_config.name}] âŒ ç²å–tokenå¤±æ•— {response.status}: {error_text}")
                        
            except Exception as e:
                logger.warning(f"[{site_config.name}] ç²å–tokenç•°å¸¸: {str(e)}")
            
            if attempt < self.max_retries:
                wait_time = min(2 ** attempt, 30)
                logger.info(f"[{site_config.name}] ç­‰å¾… {wait_time} ç§’å¾Œé‡è©¦...")
                await asyncio.sleep(wait_time)
        
        logger.error(f"[{site_config.name}] ç²å–tokenå¤±æ•—ï¼Œå·²é”åˆ°æœ€å¤§é‡è©¦æ¬¡æ•¸")
        return None
    
    async def query_site_data(self, site_config: SiteConfig, token: str, propertyvalue: str, start: str, end: str):
        """æŸ¥è©¢ç«™é»æ•¸æ“š - ä½¿ç”¨å›ºå®šç«¯é»å’Œç«™é»è‡ªå·±çš„propertyIdï¼ŒåŒ…å«æœƒå“¡ç·¨è™Ÿ"""
        session = await self.create_session()
        
        # æ§‹å»ºGraphQLæŸ¥è©¢ - ä¿®æ”¹ç‚ºåŒ…å«æœƒå“¡IDå’Œå…¶ä»–æœƒå“¡è³‡è¨Š
        graphql_query = {
            "operationName": "SearchMemberNotesWithCountAndTime",
            "query": """query SearchMemberNotesWithCountAndTime($propertyId: uuid!, $value: String!, $start: timestamptz!, $end: timestamptz!) {
  member_aggregate(
    where: {
      member_properties: { property_id: { _eq: $propertyId }, value: { _eq: $value } },
      created_at: { _gte: $start, _lte: $end },
      member_notes: { description: { _is_null: false, _neq: "" } }
    }
  ) {
    aggregate {
      count
    }
  }
  member(
    where: {
      member_properties: { property_id: { _eq: $propertyId }, value: { _eq: $value } },
      created_at: { _gte: $start, _lte: $end },
      member_notes: { description: { _is_null: false, _neq: "" } }
    }
  ) {
    id
    username
    name
    email
    created_at
    member_notes(where: { description: { _is_null: false, _neq: "" } }, order_by: { created_at: desc }) {
      id
      created_at
      description
    }
  }
}""",
            "variables": {
                "propertyId": site_config.propertyId,  # ä½¿ç”¨ç«™é»è‡ªå·±çš„propertyId
                "value": propertyvalue,                # ä½¿ç”¨POSTå‚³å…¥çš„propertyvalue
                "start": start,                        # ä½¿ç”¨POSTå‚³å…¥çš„start
                "end": end                            # ä½¿ç”¨POSTå‚³å…¥çš„end
            }
        }
        
        # è¨­ç½®è«‹æ±‚é ­ - ä½¿ç”¨Bearer token
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {token}"
        }
        
        for attempt in range(1, self.max_retries + 1):
            try:
                logger.info(f"[{site_config.name}] æŸ¥è©¢æ•¸æ“š - å˜—è©¦ {attempt}/{self.max_retries}")
                logger.info(f"[{site_config.name}] æŸ¥è©¢åƒæ•¸: propertyId={site_config.propertyId}, value={propertyvalue}")
                
                async with session.post(
                    self.query_url,  # ä½¿ç”¨å›ºå®šçš„æŸ¥è©¢ç«¯é»
                    json=graphql_query,
                    headers=headers
                ) as response:
                    
                    if response.status == 200:
                        result = await response.json()
                        
                        if result.get("errors"):
                            logger.warning(f"[{site_config.name}] GraphQLéŒ¯èª¤: {result['errors']}")
                            return {"error": "GraphQLéŒ¯èª¤", "site": site_config.name, "details": result['errors']}
                        
                        if "data" in result:
                            logger.info(f"[{site_config.name}] âœ… æˆåŠŸæŸ¥è©¢æ•¸æ“š")
                            result["site_info"] = {
                                "name": site_config.name,
                                "client_id": site_config.client_id,
                                "propertyId": site_config.propertyId,
                                "categoryId": site_config.categoryId
                            }
                            return result
                    else:
                        error_text = await response.text()
                        logger.warning(f"[{site_config.name}] æŸ¥è©¢å¤±æ•— {response.status}: {error_text}")
                        
            except Exception as e:
                logger.warning(f"[{site_config.name}] æŸ¥è©¢ç•°å¸¸: {str(e)}")
            
            if attempt < self.max_retries:
                wait_time = min(2 ** attempt, 30)
                logger.info(f"[{site_config.name}] ç­‰å¾… {wait_time} ç§’å¾Œé‡è©¦...")
                await asyncio.sleep(wait_time)
        
        logger.error(f"[{site_config.name}] æŸ¥è©¢æ•¸æ“šå¤±æ•—ï¼Œå·²é”åˆ°æœ€å¤§é‡è©¦æ¬¡æ•¸")
        return {"error": "æŸ¥è©¢å¤±æ•—", "site": site_config.name}
    
    async def process_single_site(self, site_config: SiteConfig, propertyvalue: str, start: str, end: str):
        """è™•ç†å–®å€‹ç«™é»ï¼šç²å–token -> æŸ¥è©¢æ•¸æ“š"""
        logger.info(f"[{site_config.name}] ğŸš€ é–‹å§‹è™•ç†ç«™é»")
        
        # æ­¥é©Ÿ1: ç²å–token
        token = await self.get_site_token(site_config)
        if not token:
            return {"error": "ç„¡æ³•ç²å–token", "site": site_config.name}
        
        # æ­¥é©Ÿ 2: æŸ¥è©¢æ•¸æ“š
        result = await self.query_site_data(site_config, token, propertyvalue, start, end)
        return result
    
    async def extract_all_sites_data(self, request_data: ExtractRequest):
        """ä¸¦ç™¼è™•ç†æ‰€æœ‰ç«™é»"""
        logger.info(f"ğŸ¯ é–‹å§‹è™•ç† {len(request_data.all_sites)} å€‹ç«™é»")
        logger.info(f"ğŸ“… æ™‚é–“ç¯„åœ: {request_data.start} åˆ° {request_data.end}")
        logger.info(f"ğŸ” æŸ¥è©¢å±¬æ€§å€¼: {request_data.propertyvalue}")
        
        try:
            # ä¸¦ç™¼è™•ç†æ‰€æœ‰ç«™é»
            tasks = []
            for site in request_data.all_sites:
                task = self.process_single_site(
                    site, 
                    request_data.propertyvalue, 
                    request_data.start, 
                    request_data.end
                )
                tasks.append(task)
            
            logger.info("â³ ç­‰å¾…æ‰€æœ‰ç«™é»è™•ç†å®Œæˆ...")
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # åˆä½µçµæœ
            merged_data = {
                "success_count": 0,
                "error_count": 0,
                "total_member_count": 0,
                "total_notes_count": 0,
                "sites_data": [],
                "errors": [],
                "merged_notes": [],
                "members_info": []  # æ–°å¢ï¼šæœƒå“¡è³‡è¨Šåˆ—è¡¨
            }
            
            for result in results:
                if isinstance(result, Exception):
                    merged_data["error_count"] += 1
                    merged_data["errors"].append({"error": "Exception", "message": str(result)})
                    logger.error(f"ğŸ’¥ è™•ç†ç•°å¸¸: {str(result)}")
                    continue
                
                if "error" in result:
                    merged_data["error_count"] += 1
                    merged_data["errors"].append(result)
                    logger.error(f"âŒ ç«™é» {result.get('site', 'unknown')} è™•ç†å¤±æ•—")
                    continue
                
                if "data" in result:
                    merged_data["success_count"] += 1
                    site_info = result.get("site_info", {})
                    
                    # çµ±è¨ˆæœƒå“¡æ•¸
                    member_count = 0
                    if result["data"].get("member_aggregate"):
                        member_count = result["data"]["member_aggregate"]["aggregate"]["count"]
                        merged_data["total_member_count"] += member_count
                    
                    # æ”¶é›†ç¬”è®°å’Œæœƒå“¡è³‡è¨Š
                    site_notes = []
                    site_members = []
                    
                    if result["data"].get("member"):
                        for member in result["data"]["member"]:
                            # æ”¶é›†æœƒå“¡åŸºæœ¬è³‡è¨Š
                            member_info = {
                                "member_id": member.get("id"),
                                "username": member.get("username"),
                                "name": member.get("name"),
                                "email": member.get("email"),
                                "member_created_at": member.get("created_at"),
                                "site": site_info.get("name", "unknown"),
                                "site_client_id": site_info.get("client_id"),
                                "notes_count": len(member.get("member_notes", []))
                            }
                            site_members.append(member_info)
                            merged_data["members_info"].append(member_info)
                            
                            # æ”¶é›†ç­†è¨˜ï¼ˆåŒ…å«æœƒå“¡è³‡è¨Šï¼‰
                            if member.get("member_notes"):
                                for note in member["member_notes"]:
                                    note_with_member = {
                                        "note_id": note.get("id"),
                                        "created_at": note.get("created_at"),
                                        "description": note.get("description"),
                                        "member_id": member.get("id"),
                                        "member_username": member.get("username"),
                                        "member_name": member.get("name"),
                                        "member_email": member.get("email"),
                                        "site": site_info.get("name", "unknown")
                                    }
                                    site_notes.append(note_with_member)
                                    merged_data["merged_notes"].append(note_with_member)
                    
                    merged_data["sites_data"].append({
                        "site_info": site_info,
                        "member_count": member_count,
                        "notes_count": len(site_notes),
                        "notes": site_notes,
                        "members": site_members  # æ–°å¢ï¼šè©²ç«™é»çš„æœƒå“¡åˆ—è¡¨
                    })
                    
                    logger.info(f"âœ… ç«™é» {site_info.get('name', 'unknown')} è™•ç†å®Œæˆ: {member_count} æœƒå“¡, {len(site_notes)} ç­†è¨˜")
            
            merged_data["total_notes_count"] = len(merged_data["merged_notes"])
            
            # æŒ‰æ™‚é–“æ’åºåˆä½µçš„ç­†è¨˜
            merged_data["merged_notes"].sort(key=lambda x: x.get("created_at", ""), reverse=True)
            
            # æŒ‰æœƒå“¡IDæ’åºæœƒå“¡è³‡è¨Š
            merged_data["members_info"].sort(key=lambda x: (x.get("site", ""), x.get("member_id", "")))
            
            logger.info(f"ğŸ è™•ç†å®Œæˆ: æˆåŠŸ {merged_data['success_count']}, å¤±æ•— {merged_data['error_count']}")
            logger.info(f"ğŸ“Š ç¸½è¨ˆ: {merged_data['total_member_count']} æœƒå“¡, {merged_data['total_notes_count']} ç­†è¨˜")
            
            return merged_data
            
        finally:
            await self.close_session()

extractor = DataExtractor()

@app.get("/")
async def root():
    return {
        "service": "Data Extraction Service", 
        "status": "running",
        "endpoints": {
            "token_url": extractor.token_url,
            "query_url": extractor.query_url
        }
    }

@app.get("/health")
async def health():
    return {"status": "healthy", "timestamp": datetime.now().isoformat()}

@app.post("/extract-data")
async def extract_data(request_data: ExtractRequest):
    try:
        logger.info(f"ğŸ¯ æ”¶åˆ°æ•¸æ“šæå–è«‹æ±‚")
        logger.info(f"ğŸ“‹ è«‹æ±‚åƒæ•¸: ç«™é»æ•¸={len(request_data.all_sites)}, å±¬æ€§å€¼={request_data.propertyvalue}")
        
        result = await extractor.extract_all_sites_data(request_data)
        
        return {
            "success": True,
            "timestamp": datetime.now().isoformat(),
            "request_info": {
                "propertyvalue": request_data.propertyvalue,
                "start": request_data.start,
                "end": request_data.end,
                "sites_count": len(request_data.all_sites)
            },
            "data": result
        }
        
    except Exception as e:
        logger.error(f"ğŸ’¥ è™•ç†è«‹æ±‚å‡ºéŒ¯: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    # Cloud Run ä½¿ç”¨ PORT ç’°å¢ƒè®Šé‡ï¼Œé»˜èªæ˜¯ 8080
    port = int(os.environ.get('PORT', 8080))  # æ”¹ç‚º 8080
    uvicorn.run(app, host="0.0.0.0", port=port)
