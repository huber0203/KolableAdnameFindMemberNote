from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
import aiohttp
import asyncio
import logging
from datetime import datetime
import os

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

app = FastAPI(title="Data Extraction Service", version="1.0.0")

# æ•°æ®æ¨¡å‹
class SiteConfig(BaseModel):
    name: str
    client_id: str
    key: str
    propertyId: str
    categoryId: str

class ExtractRequest(BaseModel):
    hasura_url: str
    start: str
    end: str
    propertyvalue: str
    all_sites: List[SiteConfig]

class DataExtractor:
    def __init__(self):
        self.session = None
        self.token_url = "https://api.kolable.app/api/v1/auth/token"  # å›ºå®štokenç«¯ç‚¹
        self.query_url = "https://rhdb.kolable.com/v1/graphql"       # å›ºå®šæŸ¥è¯¢ç«¯ç‚¹
        self.max_retries = 10
    
    async def create_session(self):
        if not self.session:
            timeout = aiohttp.ClientTimeout(total=30)
            self.session = aiohttp.ClientSession(timeout=timeout)
        return self.session
    
    async def close_session(self):
        if self.session:
            await self.session.close()
            self.session = None
    
    async def get_site_token(self, site_config: SiteConfig) -> Optional[str]:
        """è·å–ç«™ç‚¹token - ä½¿ç”¨å›ºå®šç«¯ç‚¹"""
        session = await self.create_session()
        
        # æŒ‰ç…§ä½ æŒ‡å®šçš„æ ¼å¼æ„å»ºè¯·æ±‚
        token_payload = {
            "clientId": site_config.client_id,
            "key": site_config.key,
            "permission": []
        }
        
        for attempt in range(1, self.max_retries + 1):
            try:
                logger.info(f"[{site_config.name}] è·å–token - å°è¯• {attempt}/{self.max_retries}")
                logger.info(f"[{site_config.name}] è¯·æ±‚æ•°æ®: {token_payload}")
                
                async with session.post(
                    self.token_url,
                    json=token_payload,
                    headers={"Content-Type": "application/json"}
                ) as response:
                    
                    if response.status == 200:
                        result = await response.json()
                        # ä¿®æ­£ï¼štokenåœ¨result.authTokené‡Œé¢
                        token = None
                        if result.get("result") and result["result"].get("authToken"):
                            token = result["result"]["authToken"]
                        elif result.get("token"):  # å¤‡ç”¨æ–¹æ¡ˆï¼Œä»¥é˜²å…¶ä»–ç«™ç‚¹æ ¼å¼ä¸åŒ
                            token = result["token"]
                        
                        if token:
                            logger.info(f"[{site_config.name}] âœ… æˆåŠŸè·å–token")
                            return token
                        else:
                            logger.warning(f"[{site_config.name}] âš ï¸ å“åº”ä¸­æ²¡æœ‰æ‰¾åˆ°token: {result}")
                    else:
                        error_text = await response.text()
                        logger.warning(f"[{site_config.name}] âŒ è·å–tokenå¤±è´¥ {response.status}: {error_text}")
                        
            except Exception as e:
                logger.warning(f"[{site_config.name}] è·å–tokenå¼‚å¸¸: {str(e)}")
            
            if attempt < self.max_retries:
                wait_time = min(2 ** attempt, 30)
                logger.info(f"[{site_config.name}] ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                await asyncio.sleep(wait_time)
        
        logger.error(f"[{site_config.name}] è·å–tokenå¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")
        return None
    
    async def query_site_data(self, site_config: SiteConfig, token: str, propertyvalue: str, start: str, end: str):
        """æŸ¥è¯¢ç«™ç‚¹æ•°æ® - ä½¿ç”¨å›ºå®šç«¯ç‚¹å’Œç«™ç‚¹è‡ªå·±çš„propertyId"""
        session = await self.create_session()
        
        # æ„å»ºGraphQLæŸ¥è¯¢ - ä½¿ç”¨ç«™ç‚¹è‡ªå·±çš„propertyId
        graphql_query = {
            "operationName": "SearchMemberNotesWithCountAndTime",
            "query": """query SearchMemberNotesWithCountAndTime($propertyId: uuid!, $value: String!, $start: timestamptz!, $end: timestamptz!) {
  member_aggregate(
    where: {
      member_properties: { property_id: { _eq: $propertyId }, value: { _eq: $value } },
      created_at: { _gte: $start, _lte: $end },
      member_notes: { description: { _is_null: false, _neq: "" } }
    }
  ) {
    aggregate {
      count
    }
  }
  member(
    where: {
      member_properties: { property_id: { _eq: $propertyId }, value: { _eq: $value } },
      created_at: { _gte: $start, _lte: $end },
      member_notes: { description: { _is_null: false, _neq: "" } }
    }
  ) {
    member_notes(where: { description: { _is_null: false, _neq: "" } }, order_by: { created_at: desc }) {
      created_at
      description
    }
  }
}""",
            "variables": {
                "propertyId": site_config.propertyId,  # ä½¿ç”¨ç«™ç‚¹è‡ªå·±çš„propertyId
                "value": propertyvalue,                # ä½¿ç”¨POSTä¼ å…¥çš„propertyvalue
                "start": start,                        # ä½¿ç”¨POSTä¼ å…¥çš„start
                "end": end                            # ä½¿ç”¨POSTä¼ å…¥çš„end
            }
        }
        
        # è®¾ç½®è¯·æ±‚å¤´ - ä½¿ç”¨Bearer token
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {token}"
        }
        
        for attempt in range(1, self.max_retries + 1):
            try:
                logger.info(f"[{site_config.name}] æŸ¥è¯¢æ•°æ® - å°è¯• {attempt}/{self.max_retries}")
                logger.info(f"[{site_config.name}] æŸ¥è¯¢å‚æ•°: propertyId={site_config.propertyId}, value={propertyvalue}")
                
                async with session.post(
                    self.query_url,  # ä½¿ç”¨å›ºå®šçš„æŸ¥è¯¢ç«¯ç‚¹
                    json=graphql_query,
                    headers=headers
                ) as response:
                    
                    if response.status == 200:
                        result = await response.json()
                        
                        if result.get("errors"):
                            logger.warning(f"[{site_config.name}] GraphQLé”™è¯¯: {result['errors']}")
                            return {"error": "GraphQLé”™è¯¯", "site": site_config.name, "details": result['errors']}
                        
                        if "data" in result:
                            logger.info(f"[{site_config.name}] âœ… æˆåŠŸæŸ¥è¯¢æ•°æ®")
                            result["site_info"] = {
                                "name": site_config.name,
                                "client_id": site_config.client_id,
                                "propertyId": site_config.propertyId,
                                "categoryId": site_config.categoryId
                            }
                            return result
                    else:
                        error_text = await response.text()
                        logger.warning(f"[{site_config.name}] æŸ¥è¯¢å¤±è´¥ {response.status}: {error_text}")
                        
            except Exception as e:
                logger.warning(f"[{site_config.name}] æŸ¥è¯¢å¼‚å¸¸: {str(e)}")
            
            if attempt < self.max_retries:
                wait_time = min(2 ** attempt, 30)
                logger.info(f"[{site_config.name}] ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                await asyncio.sleep(wait_time)
        
        logger.error(f"[{site_config.name}] æŸ¥è¯¢æ•°æ®å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")
        return {"error": "æŸ¥è¯¢å¤±è´¥", "site": site_config.name}
    
    async def process_single_site(self, site_config: SiteConfig, propertyvalue: str, start: str, end: str):
        """å¤„ç†å•ä¸ªç«™ç‚¹ï¼šè·å–token -> æŸ¥è¯¢æ•°æ®"""
        logger.info(f"[{site_config.name}] ğŸš€ å¼€å§‹å¤„ç†ç«™ç‚¹")
        
        # æ­¥éª¤1: è·å–token
        token = await self.get_site_token(site_config)
        if not token:
            return {"error": "æ— æ³•è·å–token", "site": site_config.name}
        
        # æ­¥éª¤ 2: æŸ¥è¯¢æ•°æ®
        result = await self.query_site_data(site_config, token, propertyvalue, start, end)
        return result
    
    async def extract_all_sites_data(self, request_data: ExtractRequest):
        """å¹¶å‘å¤„ç†æ‰€æœ‰ç«™ç‚¹"""
        logger.info(f"ğŸ¯ å¼€å§‹å¤„ç† {len(request_data.all_sites)} ä¸ªç«™ç‚¹")
        logger.info(f"ğŸ“… æ—¶é—´èŒƒå›´: {request_data.start} åˆ° {request_data.end}")
        logger.info(f"ğŸ” æŸ¥è¯¢å±æ€§å€¼: {request_data.propertyvalue}")
        
        try:
            # å¹¶å‘å¤„ç†æ‰€æœ‰ç«™ç‚¹
            tasks = []
            for site in request_data.all_sites:
                task = self.process_single_site(
                    site, 
                    request_data.propertyvalue, 
                    request_data.start, 
                    request_data.end
                )
                tasks.append(task)
            
            logger.info("â³ ç­‰å¾…æ‰€æœ‰ç«™ç‚¹å¤„ç†å®Œæˆ...")
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # åˆå¹¶ç»“æœ
            merged_data = {
                "success_count": 0,
                "error_count": 0,
                "total_member_count": 0,
                "total_notes_count": 0,
                "sites_data": [],
                "errors": [],
                "merged_notes": []
            }
            
            for result in results:
                if isinstance(result, Exception):
                    merged_data["error_count"] += 1
                    merged_data["errors"].append({"error": "Exception", "message": str(result)})
                    logger.error(f"ğŸ’¥ å¤„ç†å¼‚å¸¸: {str(result)}")
                    continue
                
                if "error" in result:
                    merged_data["error_count"] += 1
                    merged_data["errors"].append(result)
                    logger.error(f"âŒ ç«™ç‚¹ {result.get('site', 'unknown')} å¤„ç†å¤±è´¥")
                    continue
                
                if "data" in result:
                    merged_data["success_count"] += 1
                    site_info = result.get("site_info", {})
                    
                    # ç»Ÿè®¡ä¼šå‘˜æ•°
                    member_count = 0
                    if result["data"].get("member_aggregate"):
                        member_count = result["data"]["member_aggregate"]["aggregate"]["count"]
                        merged_data["total_member_count"] += member_count
                    
                    # æ”¶é›†ç¬”è®°
                    site_notes = []
                    if result["data"].get("member"):
                        for member in result["data"]["member"]:
                            if member.get("member_notes"):
                                for note in member["member_notes"]:
                                    note_with_site = {
                                        **note,
                                        "site": site_info.get("name", "unknown")
                                    }
                                    site_notes.append(note_with_site)
                                    merged_data["merged_notes"].append(note_with_site)
                    
                    merged_data["sites_data"].append({
                        "site_info": site_info,
                        "member_count": member_count,
                        "notes_count": len(site_notes),
                        "notes": site_notes
                    })
                    
                    logger.info(f"âœ… ç«™ç‚¹ {site_info.get('name', 'unknown')} å¤„ç†å®Œæˆ: {member_count} ä¼šå‘˜, {len(site_notes)} ç¬”è®°")
            
            merged_data["total_notes_count"] = len(merged_data["merged_notes"])
            
            # æŒ‰æ—¶é—´æ’åºåˆå¹¶çš„ç¬”è®°
            merged_data["merged_notes"].sort(key=lambda x: x.get("created_at", ""), reverse=True)
            
            logger.info(f"ğŸ å¤„ç†å®Œæˆ: æˆåŠŸ {merged_data['success_count']}, å¤±è´¥ {merged_data['error_count']}")
            logger.info(f"ğŸ“Š æ€»è®¡: {merged_data['total_member_count']} ä¼šå‘˜, {merged_data['total_notes_count']} ç¬”è®°")
            
            return merged_data
            
        finally:
            await self.close_session()

extractor = DataExtractor()

@app.get("/")
async def root():
    return {
        "service": "Data Extraction Service", 
        "status": "running",
        "endpoints": {
            "token_url": extractor.token_url,
            "query_url": extractor.query_url
        }
    }

@app.get("/health")
async def health():
    return {"status": "healthy", "timestamp": datetime.now().isoformat()}

@app.post("/extract-data")
async def extract_data(request_data: ExtractRequest):
    try:
        logger.info(f"ğŸ¯ æ”¶åˆ°æ•°æ®æå–è¯·æ±‚")
        logger.info(f"ğŸ“‹ è¯·æ±‚å‚æ•°: ç«™ç‚¹æ•°={len(request_data.all_sites)}, å±æ€§å€¼={request_data.propertyvalue}")
        
        result = await extractor.extract_all_sites_data(request_data)
        
        return {
            "success": True,
            "timestamp": datetime.now().isoformat(),
            "request_info": {
                "propertyvalue": request_data.propertyvalue,
                "start": request_data.start,
                "end": request_data.end,
                "sites_count": len(request_data.all_sites)
            },
            "data": result
        }
        
    except Exception as e:
        logger.error(f"ğŸ’¥ å¤„ç†è¯·æ±‚å‡ºé”™: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    # Cloud Run ä½¿ç”¨ PORT ç¯å¢ƒå˜é‡ï¼Œé»˜è®¤æ˜¯ 8080
    port = int(os.environ.get('PORT', 8080))  # æ”¹ä¸º 8080
    uvicorn.run(app, host="0.0.0.0", port=port)
